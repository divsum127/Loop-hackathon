#!/usr/bin/env python3
"""
Ensemble Inference - Combine Base and VocabFine Models

This script implements the simplest and most effective improvement:
combining predictions from multiple models for more robust results.

Expected improvement: +2-3% AUROC
"""

import torch
import numpy as np
import json
import argparse
from pathlib import Path
from datetime import datetime
from .ct_clip_inference import CTClipInferenceSingle


def ensemble_inference(
    volume_path,
    models={'base': 'models/ct_clip_v2.pt', 'vocabfine': 'models/ct_vocabfine_v2.pt'},
    weights={'base': 0.4, 'vocabfine': 0.6},  # Weight VocabFine higher
    threshold=0.3,
    device='cuda'
):
    """
    Run ensemble inference combining multiple models.
    
    Args:
        volume_path: Path to CT volume
        models: Dict of {model_name: model_path}
        weights: Dict of {model_name: weight} (should sum to 1.0)
        threshold: Detection threshold
        device: cuda or cpu
    
    Returns:
        Dictionary with ensemble predictions and individual model results
    """
    print("=" * 80)
    print("ENSEMBLE INFERENCE")
    print("=" * 80)
    print(f"Volume: {volume_path}")
    print(f"Models: {list(models.keys())}")
    print(f"Weights: {weights}")
    print(f"Threshold: {threshold}")
    print("=" * 80)
    
    # Normalize weights
    total_weight = sum(weights.values())
    normalized_weights = {k: v/total_weight for k, v in weights.items()}
    
    # Run inference for each model
    all_predictions = {}
    inference_times = {}
    
    for model_name, model_path in models.items():
        print(f"\n{'='*80}")
        print(f"Running {model_name} model...")
        print(f"{'='*80}")
        
        inferencer = CTClipInferenceSingle(
            model_path=model_path,
            device=device,
            threshold=threshold
        )
        
        results = inferencer.infer_single_volume(
            volume_path=volume_path,
            verbose=True
        )
        
        all_predictions[model_name] = results['predictions']
        inference_times[model_name] = results.get('inference_time_seconds', 0)
    
    # Compute ensemble predictions
    print(f"\n{'='*80}")
    print("COMPUTING ENSEMBLE PREDICTIONS")
    print(f"{'='*80}")
    
    pathologies = list(next(iter(all_predictions.values())).keys())
    ensemble_predictions = {}
    
    for pathology in pathologies:
        weighted_sum = 0
        for model_name, preds in all_predictions.items():
            weight = normalized_weights[model_name]
            weighted_sum += weight * preds[pathology]
        
        ensemble_predictions[pathology] = weighted_sum
    
    # Determine positive findings
    positive_findings = [
        pathology for pathology, prob in ensemble_predictions.items()
        if prob >= threshold
    ]
    
    # Results summary
    print(f"\n{'='*80}")
    print("ENSEMBLE RESULTS")
    print(f"{'='*80}\n")
    
    print(f"{'Pathology':<40} {'Base':<10} {'VocabFine':<10} {'Ensemble':<10} {'Status'}")
    print("-" * 90)
    
    for pathology in sorted(pathologies):
        base_prob = all_predictions['base'][pathology]
        vocab_prob = all_predictions['vocabfine'][pathology]
        ens_prob = ensemble_predictions[pathology]
        
        status = "‚úì POS" if ens_prob >= threshold else "‚úó neg"
        
        # Highlight improvements
        marker = ""
        if abs(ens_prob - base_prob) > 0.1 and abs(ens_prob - vocab_prob) > 0.1:
            marker = " üéØ"  # Ensemble significantly different
        
        print(f"{pathology:<40} {base_prob:.3f}     {vocab_prob:.3f}     {ens_prob:.3f}     {status}{marker}")
    
    print(f"\n{'='*80}")
    print(f"Positive findings: {len(positive_findings)}/{len(pathologies)}")
    print(f"Total inference time: {sum(inference_times.values()):.2f}s")
    print(f"{'='*80}")
    
    # Show which predictions changed
    base_positives = set(p for p, v in all_predictions['base'].items() if v >= threshold)
    vocab_positives = set(p for p, v in all_predictions['vocabfine'].items() if v >= threshold)
    ensemble_positives = set(positive_findings)
    
    only_ensemble = ensemble_positives - base_positives - vocab_positives
    if only_ensemble:
        print(f"\n‚ú® Ensemble detected (both models missed):")
        for p in only_ensemble:
            print(f"  ‚Ä¢ {p}: {ensemble_predictions[p]:.3f}")
    
    disagreements = base_positives.symmetric_difference(vocab_positives)
    if disagreements:
        print(f"\n‚ö†Ô∏è  Models disagreed on {len(disagreements)} findings:")
        for p in disagreements:
            print(f"  ‚Ä¢ {p}: Base={all_predictions['base'][p]:.3f}, "
                  f"VocabFine={all_predictions['vocabfine'][p]:.3f}, "
                  f"Ensemble={ensemble_predictions[p]:.3f}")
    
    # Prepare results dict
    results = {
        'volume_path': str(volume_path),
        'timestamp': datetime.now().isoformat(),
        'models': {k: str(v) for k, v in models.items()},
        'weights': normalized_weights,
        'threshold': threshold,
        'device': device,
        'individual_predictions': all_predictions,
        'ensemble_predictions': ensemble_predictions,
        'positive_findings': positive_findings,
        'inference_times': inference_times,
        'total_inference_time': sum(inference_times.values())
    }
    
    return results


def main():
    parser = argparse.ArgumentParser(
        description='Ensemble inference combining multiple CT-CLIP models'
    )
    parser.add_argument(
        '--volume-path',
        required=True,
        help='Path to CT volume'
    )
    parser.add_argument(
        '--base-model',
        default='models/ct_clip_v2.pt',
        help='Path to base model'
    )
    parser.add_argument(
        '--vocabfine-model',
        default='models/ct_vocabfine_v2.pt',
        help='Path to VocabFine model'
    )
    parser.add_argument(
        '--base-weight',
        type=float,
        default=0.4,
        help='Weight for base model (default: 0.4)'
    )
    parser.add_argument(
        '--vocabfine-weight',
        type=float,
        default=0.6,
        help='Weight for VocabFine model (default: 0.6)'
    )
    parser.add_argument(
        '--threshold',
        type=float,
        default=0.3,
        help='Detection threshold (default: 0.3)'
    )
    parser.add_argument(
        '--output-dir',
        default='results',
        help='Output directory for results'
    )
    parser.add_argument(
        '--device',
        default='cuda',
        choices=['cuda', 'cpu'],
        help='Device to run inference on'
    )
    
    args = parser.parse_args()
    
    # Check if models exist
    base_path = Path(args.base_model)
    vocab_path = Path(args.vocabfine_model)
    
    if not base_path.exists():
        print(f"Error: Base model not found: {args.base_model}")
        print("Download it with: python download_model.py --model-type base")
        return 1
    
    if not vocab_path.exists():
        print(f"Error: VocabFine model not found: {args.vocabfine_model}")
        print("Download it with: python download_model.py --model-type vocabfine")
        return 1
    
    # Run ensemble inference
    results = ensemble_inference(
        volume_path=args.volume_path,
        models={'base': args.base_model, 'vocabfine': args.vocabfine_model},
        weights={'base': args.base_weight, 'vocabfine': args.vocabfine_weight},
        threshold=args.threshold,
        device=args.device
    )
    
    # Save results
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    volume_name = Path(args.volume_path).stem.replace('.nii', '')
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    output_file = output_dir / f"{volume_name}_ensemble_{timestamp}.json"
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\n‚úì Results saved to: {output_file}")
    
    # Generate report
    print(f"\n{'='*80}")
    print("üí° TIP: Generate a report with:")
    print(f"  python report_generator.py --paragraph {output_file}")
    print(f"{'='*80}")
    
    return 0


if __name__ == '__main__':
    import sys
    sys.exit(main())
